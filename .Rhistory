subset <- regsubsets(Y~poly(X,10,raw=TRUE),data=data)
summary(subset)
knitr::opts_chunk$set(echo = TRUE)
# Libraries
library(RCurl)
library(ggplot2)
library(skimr)
# importing data set
MI = read.csv("./MI.csv")
hist(MI$AGE, main = "Histogram of Age", xlab = "Age", col = "lightblue", border = "black")
hist(MI$SEX, main - "Histogram of Sex", xlab = "Sex", col = "lightblue", border = "black")
hist(MI$SEX, main - "Histogram of Sex", xlab = "Sex", col = "lightblue", border = "black")
hist(MI$SEX, main = "Histogram of Sex", xlab = "Sex", col = "lightblue", border = "black")
hist(MI$LET_IS, main = "Histogram of LET_IS", xlab = "LET_IS", col = "lightblue", border = "black")
barplot(MI$LET_IS, main = "Histogram of LET_IS", xlab = "LET_IS", col = "lightblue", border = "black")
hist
hist(MI$LET_IS, main = "Histogram of LET_IS", xlab = "LET_IS", col = "lightblue", border = "black")
library(caret)
library(rpart)
library(rpart.plot)
# Load packages
library(rpart)
# Bootstrapping or Cross-validation?
set.seed(33)
# cross-validation
trainIndex <- createDataPartition(MI$LET_IS, p = .8,
list = FALSE,
times = 1)
MI_train <- MI[ trainIndex,]
MI_test  <- MI[-trainIndex,]
install.packages("glmnet")
X <- model.matrix(LET_IS ~ ., data = MI_train)[,-1]
library(glmnet)
y <- MI_train$LET_IS
# Dealing with missing data: imputation or deletion?
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(rpart)
library(RCurl)
library(ggplot2)
library(skimr)
library(caret)
# importing data set
MI = read.csv("./MI.csv")
#head(MI)
#summary(MI)
# checking for missing values
#sum(is.na(MI))
# How to handle missing data?
hist(MI$AGE, main = "Histogram of Age", xlab = "Age", col = "lightblue", border = "black")
hist(MI$SEX, main = "Histogram of Sex", xlab = "Sex", col = "lightblue", border = "black")
hist(MI$LET_IS, main = "Histogram of LET_IS", xlab = "LET_IS", col = "lightblue", border = "black")
# I think anything other than zero is dead (1-7) on LET_IS variable.
library(dplyr)
X <- select(data, -LET_IS)
X <- select(MI, -LET_IS)
y <- MI$LET_IS
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(rpart)
library(RCurl)
library(ggplot2)
library(skimr)
library(caret)
# importing data set
MI = read.csv("./MI.csv")
#head(MI)
#summary(MI)
# checking for missing values
#sum(is.na(MI))
# How to handle missing data?
hist(MI$AGE, main = "Histogram of Age", xlab = "Age", col = "lightblue", border = "black")
hist(MI$SEX, main = "Histogram of Sex", xlab = "Sex", col = "lightblue", border = "black")
hist(MI$LET_IS, main = "Histogram of LET_IS", xlab = "LET_IS", col = "lightblue", border = "black")
# I think anything other than zero is dead (1-7) on LET_IS variable.
library(glmnet)
library(dplyr)
X <- select(MI, -LET_IS)
y <- MI$LET_IS
# Ridge regression model
ridge_model <- glmnet(X, y, alpha=0)
View(MI)
for (col in names(MI)) {
# Identify missing values in the column
missing_indices <- is.na(MI[[col]])
# Check if there are any missing values to impute
if (any(missing_indices)) {
# Formulate a model formula dynamically excluding the column with missing data
formula <- as.formula(paste(col, "~ ."))
# Exclude the column itself from predictors
predictors <- setdiff(names(MI), col)
# Fit model using available cases
fit <- lm(formula, data=data, subset=!missing_indices)
# Predict missing values using the fitted model
MI[missing_indices, col] <- predict(fit, newdata = data[missing_indices, predictors, drop = FALSE])
}
}
for (col in names(MI)) {
# Identify missing values in the column
missing_indices <- is.na(MI[[col]])
# Check if there are any missing values to impute
if (any(missing_indices)) {
# Formulate a model formula dynamically excluding the column with missing data
formula <- as.formula(paste(col, "~ ."))
# Exclude the column itself from predictors
predictors <- setdiff(names(MI), col)
# Fit model using available cases
fit <- lm(formula, data=MI, subset=!missing_indices)
# Predict missing values using the fitted model
MI[missing_indices, col] <- predict(fit, newdata = MI[missing_indices, predictors, drop = FALSE])
}
}
View(MI)
names(MI)
is.na(IBS_NASL)
is.na('IBS_NASL')
is.na(MI$IBS_NASL)
is.na(MI[['IBS_NASL']])
missing_i = is.na(MI[['IBS_NASL']])
any(missing_i))
any(missing_i)
for (col in names(MI)) {
# Identify missing values in the column
missing_indices <- is.na(MI[[col]])
# Check if there are any missing values to impute
if (any(missing_indices)) {
# Formulate a model formula dynamically excluding the column with missing data
formula <- as.formula(paste(col, "~ ."))
formula
# Exclude the column itself from predictors
predictors <- setdiff(names(MI), col)
# Fit model using available cases
fit <- lm(formula, data=MI, subset=!missing_indices)
# Predict missing values using the fitted model
MI[missing_indices, col] <- predict(fit, newdata = MI[missing_indices, predictors, drop = FALSE])
}
}
setwd("~/Desktop/2nd_semester/statistical_bioinformatics")
load('VIJVER.Rdata')
dim(data) # 188 4949 ==> 4948 variables for only 188 data points
#nearly 1*10^6 datapoints!
colnames(data)[1:100]
npts = nrow(data) # number of data points
nvars = ncol(data) - 1 # number of variables (each var is a gene expr)
geneExpr=as.matrix(data[,-1])#no first column
meta= data[,1]#response
summary(meta)
index=9
plot(geneExpr[,index]~meta)
model=glm(meta~geneExpr[,index],family="binomial")
summary(model)
#4. Demonstrate if collinearity occurs between genes in this dataset
correlationMatrix=(cor(geneExpr))
MT1=apply(correlationMatrix>0.9,2,sum)
max(MT1)
length(which(MT1>1))/length(MT1)
#7% of genes have at least one gene for which correlation >0.9!
MT2=apply(correlationMatrix>0.5,2,sum)
length(which(MT2>1))/length(MT2)
model=lm(geneExpr[,3]~geneExpr[,4])
summary(model)
cor(geneExpr[,3],geneExpr[,4])
plot(geneExpr[,3]~geneExpr[,4])
abline(model)
#make test and training set
set.seed(1)
train=sample(1:nrow(geneExpr), nrow(geneExpr)*2/3)
test=(-train)
#do a lasso!
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(y=meta[train],x=(geneExpr[train,]),alpha=1,family="binomial")#note error family
plot(lasso.mod)
cv.lasso=cv.glmnet(geneExpr[train ,],meta[train],alpha=1,family="binomial")
plot(cv.lasso)
lasso.pred=predict(lasso.mod,s=cv.lasso$lambda.min,newx=geneExpr[test,],type="response")
plot(lasso.pred~meta[test])
library(glmnet)
library(pROC)
library(doBy)
library(pls)
install.packages("pROC")
library(glmnet)
library(pROC)
library(doBy)
library(pls)
install.packages("pls")
library(glmnet)
library(pROC)
library(doBy)
library(pls)
load('VIJVER.Rdata')
dim(data) # 188 4949 ==> 4948 variables for only 188 data points
#nearly 1*10^6 datapoints!
colnames(data)[1:100]
npts = nrow(data) # number of data points
nvars = ncol(data) - 1 # number of variables (each var is a gene expr)
geneExpr=as.matrix(data[,-1])#no first column
meta= data[,1]#response
summary(meta)
index=9
plot(geneExpr[,index]~meta)
model=glm(meta~geneExpr[,index],family="binomial")
summary(model)
#4. Demonstrate if collinearity occurs between genes in this dataset
correlationMatrix=(cor(geneExpr))
MT1=apply(correlationMatrix>0.9,2,sum)
max(MT1)
length(which(MT1>1))/length(MT1)
#7% of genes have at least one gene for which correlation >0.9!
MT2=apply(correlationMatrix>0.5,2,sum)
length(which(MT2>1))/length(MT2)
model=lm(geneExpr[,3]~geneExpr[,4])
summary(model)
cor(geneExpr[,3],geneExpr[,4])
plot(geneExpr[,3]~geneExpr[,4])
abline(model)
#make test and training set
set.seed(1)
train=sample(1:nrow(geneExpr), nrow(geneExpr)*2/3)
test=(-train)
#do a lasso!
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(y=meta[train],x=(geneExpr[train,]),alpha=1,family="binomial")#note error family
plot(lasso.mod)
cv.lasso=cv.glmnet(geneExpr[train ,],meta[train],alpha=1,family="binomial")
plot(cv.lasso)
lasso.pred=predict(lasso.mod,s=cv.lasso$lambda.min,newx=geneExpr[test,],type="response")
plot(lasso.pred~meta[test])
pred=rep("DM",length(meta[test]))
pred[lasso.pred>0.5]="NODM" #set threshold of 0.5
table(meta[test],pred)
performanceLasso=length(which(pred==meta[test]))/length(meta[test])
performanceLasso #accuracy, 0.71
#Receiver operating statistics!
rocLasso=roc((meta[test]), lasso.pred[,1]) #AUC 0.7357
plot(rocLasso)
#selected genes
vals=predict(lasso.mod,s=cv.lasso$lambda.min,type="coefficients")
selected=colnames(geneExpr)[vals@i]
#do a ridge!
ridge.mod=glmnet(y=meta[train],x=(geneExpr[train,]),alpha=0,family="binomial")
plot(ridge.mod)
ridge.cv=cv.glmnet(geneExpr[train ,],meta[train],alpha=0,family="binomial")
plot(ridge.cv)
ridge.pred=predict(ridge.mod,s=ridge.cv$lambda.min,newx=geneExpr[test,],type="response")
pred=rep("DM",length(meta[test]))
pred[ridge.pred>0.5]="NODM"
table(meta[test],pred)
performanceRidge=length(which(pred==meta[test]))/length(meta[test])
performanceRidge #accuracy, 0.63
#Receiver operating statistics!
rocRidge=roc((meta[test]), ridge.pred[,1]) #AUC 0.7173
plot(rocRidge)
#do a pcr
metaRef= meta=="NODM"
DF=data.frame(meta=metaRef,geneExpr)
pcr.mod <- pcr(meta ~ .,family=binomial(link=logit), data=DF[train,], subset=train, scale=TRUE,validation="CV")
summary(pcr.mod)
validationplot(pcr.mod,val.type="RMSEP")#14 components
pcr.pred <- predict(pcr.mod,data[test,], ncomp=14,type="response")
pred=rep("DM",length(meta[test]))
hist(pcr.pred)#Oops need pls-glm plsRglm for logistic regression!
library(Compositional)
install.packages("Compositional")
install.packages("rpart")
install.packages("rpart.plot")  # For enhanced plotting
library(rpart)
library(rpart.plot)
library(tree)
install.packages("tree")
library(tree)
View(data)
tree_model <- tree(meta ~ ., train)
train_df = data.frame(train)
tree_model <- tree(meta ~ ., train)
tree_model <- tree(meta ~ ., train_df)
train_df = data.frame(train)
tree_model <- tree(meta ~ ., train_df)
tree_model <- tree(meta ~ ., data)
library(tree)
tree_model <- tree(meta ~ ., data)
summary(tree_model)
tree_model <- tree(meta ~ ., train)
train_df = data.frame(meta = meta[train], geneExpr[train,])
tree_model <- tree(meta ~ ., train_df)
summary(tree_model)
plot(tree_model)
text(tree.carseats, pretty = 0)
plot(tree_model)
text(tree.carseats, pretty = 0)
text(tree_model, pretty = 0)
#Eval of tree
tree_pred <- predict(tree_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], tree_pred)
# Pruning Tree
cv_tree <- cv.tree(tree_model, FUN = prune.misclass)
# Pruning Tree
cv_tree <- cv.tree(tree_model, FUN = prune.misclass)
tree_pred1 <- predict(cv_tree, data.frame(geneExpr[test,]), type = "class")
names(cv_tree)
cv_tree
> text(prunetree, pretty = 0)
prunetree <- prune.misclass(tree_pred, best = 2)
prunetree <- prune.misclass(tree_model, best = 2)
plot(prunetree)
text(prunetree, pretty = 0)
# Check performace of pruned tree
tree_pred2 <- predict(prunetree, data.frame(geneExpr[test,]), type = "class")
table(meta[test], tree_pred2)
install.packages("randomForest")
bagging_model
bagging_model <- randomForest(meta ~ ., data = train_df, ntree = 500)
# Bagging of Trees
library(randomForest)
bagging_model <- randomForest(meta ~ ., data = train_df, ntree = 500)
bagging_model
bagging_pred <- predict(bagging_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], bagging_pred)
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
# *** Random Forest ***
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
set.seed(1)
bagging_model <- randomForest(meta ~ ., data = train_df, ntree = 500)
bagging_pred <- predict(bagging_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], bagging_pred)
# *** Random Forest ***
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
# *** Random Forest ***
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
set.seed(1)
# *** Random Forest ***
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
# *** Random Forest ***
forest_model <- randomForest(meta ~ ., mtry= 12, data = train_df, ntree = 500)
forest_pred <- predict(forest_model, data.frame(geneExpr[test,]), type = "class")
table(meta[test], forest_pred)
boosting_model <- gbm(meta ~ ., data = train_df, distribution = "gaussian", n.trees = 500, interaction.depth = 4, shrinkage = 0.01)
boosting_pred <- predict(boosting_model, data.frame(geneExpr[test,]), n.trees = 500)
# *** Boosting ***
library(gbm)
boosting_model <- gbm(meta ~ ., data = train_df, distribution = "gaussian", n.trees = 500, interaction.depth = 4, shrinkage = 0.01)
boosting_pred <- predict(boosting_model, data.frame(geneExpr[test,]), n.trees = 500)
table(meta[test], boosting_pred)
summary(boosting_model)
plot(boosting_model, i = "rm")
# importing data set
#MI = read.csv("./MI.csv")
MI = load("./MI.RData")
MI
setwd("~/Desktop/2nd_semester/statistical_bioinformatics/assignment2")
# importing data set
#MI = read.csv("./MI.csv")
MI = load("./MI.RData")
MI
# importing data set
#MI = read.csv("./MI.csv")
load("./MI.RData")
MI
View(MI)
sum(isNA(MI))
sum(is.na(MI))
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(rpart)
library(RCurl)
library(ggplot2)
library(skimr)
library(caret)
# importing data set
#MI = read.csv("./MI.csv")
load("./MI.RData")
MI
sum(is.na(MI))
# Removing missing values
MI = MI[complete.cases(MI),]
trainIndex <- createDataPartition(MI$LET_IS, p = .8,
list = FALSE,
times = 1)
MI_train <- MI[ trainIndex,]
MI_test  <- MI[-trainIndex,]
library(glmnet)
library(dplyr)
X <- select(MI_train, -LET_IS)
y <- MI_train$LET_IS
ridge_model <- glmnet(X, y, alpha=0)
y
# Linear model
regfit.fwd <- regsubsets(LET_IS ∼ ., data = MI, nvmax = 19, method = "forward")
X <- select(MI_train, -LET_IS)
y <- as.numeric(MI_train$LET_IS)
# Linear model
regfit.fwd <- regsubsets(LET_IS ∼ ., data = MI, nvmax = 19, method = "forward")
head(MI)
MI$LET_IS <- as.numeric(as.character(data$LET_IS))
# Convert factor columns to numeric
MI$LET_IS <- as.numeric(ifelse(MI$LET_IS == FALSE, 0, 1)
MI$SEX <- as.numeric(as.character(data$SEX))
# Convert factor columns to numeric
MI$LET_IS <- as.numeric(ifelse(MI$LET_IS == FALSE, 0, 1))
MI$SEX <- as.numeric(as.character(data$SEX))
MI$SEX <- as.numeric(data$SEX)
# Linear model
regfit.fwd <- regsubsets(LET_IS ∼ ., data = MI, nvmax = 19, method = "forward")
MI$SEX <- as.numeric(data$SEX)
# importing data set
#MI = read.csv("./MI.csv")
load("./MI.RData")
# Removing missing values
MI = MI[complete.cases(MI),]
trainIndex <- createDataPartition(MI$LET_IS, p = .8,
list = FALSE,
times = 1)
MI_train <- MI[ trainIndex,]
MI_test  <- MI[-trainIndex,]
library(glmnet)
library(dplyr)
# Linear model
regfit.fwd <- regsubsets(LET_IS ∼ ., data = MI_train, nvmax = 19, method = "forward")
# importing data set
#MI = read.csv("./MI.csv")
load("./MI.RData")
# importing data set
#MI = read.csv("./MI.csv")
load("./MI.RData")
# Removing missing values
MI = MI[complete.cases(MI),]
# Linear model
regfit.fwd <- regsubsets(LET_IS ∼ ., data = MI, nvmax = 19, method = "forward")
library(leaps)
# Linear model
regfit.fwd <- regsubsets(LET_IS ∼ ., data = MI, nvmax = 19, method = "forward")
trainIndex <- createDataPartition(MI$LET_IS, p = .8,
list = FALSE,
times = 1)
MI_train <- MI[ trainIndex,]
MI_test  <- MI[-trainIndex,]
# Linear model
regfit.fwd <- regsubsets(LET_IS ~ ., data = MI, nvmax = 19, method = "forward")
summary(regfit.fwd)
library(glmnet)
library(dplyr)
library(leaps)
# Convert factor columns to numeric
#MI$LET_IS <- as.numeric(ifelse(MI$LET_IS == FALSE, 0, 1))
#MI$SEX <- as.numeric(data$SEX)
X <- select(MI_train, -LET_IS)
y <- as.numeric(MI_train$LET_IS)
# Linear model
regfit.fwd <- regsubsets(LET_IS ~ ., data = MI, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(LET_IS ~ ., data = MI_train,
nvmax = 19, method = "backward") > summary(regfit.bwd)
regfit.bwd <- regsubsets(LET_IS ~ ., data = MI_train, nvmax = 19, method = "backward")
summary(regfit.bwd)
# Ridge regression model
ridge_model <- glmnet(X, y, alpha=0)
plot(ridge_model, xvar="lambda")
# Lasso regression model
lasso_model <- glmnet(X, y, alpha=1)
# anova to evaluate best model
anova(regfit.fwd, regfit.bwd, ridge_model, lasso_model))
# anova to evaluate best model
anova(regfit.fwd, regfit.bwd, ridge_model, lasso_model)
View(regfit.bwd)
# anova to evaluate best model
anova( ridge_model, lasso_model)
# Evaluate best model
test_matrix <- model.matrix(LET_IS ~ ., data = MI_test)
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(rpart)
library(RCurl)
library(ggplot2)
library(skimr)
library(caret)
# importing data set
load("./MI.RData")
#sum(is.na(MI))
plot(ridge_model, xvar="lambda")
View(MI_test)
set.seed(33)
# Random Forest model
forest_model <- randomForest(LET_IS ~ ., mtry= 12, data = MI_train, ntree = 500)
# Random Forest model
forest_model <- randomForest(LET_IS ~ ., mtry= 12, data = MI_train, ntree = 500)
forest_pred <- predict(forest_model, MI_test, type = "class")
forest_pred <- predict(forest_model, MI_test, type = "class")
table(MI_test$LET_IS, forest_pred)
# ROC curve
library(pROC)
roc_obj <- roc(MI_test$LET_IS, forest_pred)
help(roc)
help("as.numeric")
forest_pred
roc_obj <- roc(MI_test$LET_IS, as.numeric(forest_pred))
plot(roc_obj, col = "blue")
