---
title: "Stats Project 2"
author: "Theodoro Gasperin Terra Camargo"
class: "Statistical Methods for Bioinformatics"
date: "2024-05-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(rpart)
library(RCurl)
library(ggplot2)
library(skimr)
library(caret)

# importing data set
load("./MI.RData")

```

### Question 1

Study and describe the predictor variables. Do you see any issues that are relevant for making predictions? Make sure to discuss the dimensionality of the data and the implication on fitting models.


```{r data_exploration, echo=FALSE}
# head(MI)
# summary(MI)
# pairs()


# GAM (general additive models)? 
#gam()
# predict(gam1, se=TRUE, col= "purple")

# checking for missing values
#sum(is.na(MI))

```

```{r missing_data, echo=FALSE}

# Predictive Imputation

# Removing missing values
MI = MI[complete.cases(MI),]

```


```{r split_data, echo=FALSE}
# Splitting data into training and testing sets
trainIndex <- createDataPartition(MI$LET_IS, p = .8, 
                                  list = FALSE, 
                                  times = 1)
MI_train <- MI[ trainIndex,]
MI_test  <- MI[-trainIndex,]
```

### Question 2  

Fit and compare an appropriate unconstrained linear model, as well as lasso and ridge regression models. Discuss what you find. What is an appropriate base-level of performance to compare your models to?

```{r linear_models, echo=FALSE}

library(glmnet)
library(dplyr)
library(leaps)

X <- select(MI_train, -LET_IS)
y <- as.numeric(MI_train$LET_IS)

# Linear model
regfit.fwd <- regsubsets(LET_IS ~ ., data = MI, nvmax = 19, method = "forward")
#summary(regfit.fwd)

regfit.bwd <- regsubsets(LET_IS ~ ., data = MI_train, nvmax = 19, method = "backward") 
#summary(regfit.bwd)

# Ridge regression model
ridge_model <- glmnet(X, y, alpha=0)
plot(ridge_model, xvar="lambda")

# Lasso regression model
lasso_model <- glmnet(X, y, alpha=1)
plot(lasso_model, xvar="lambda")


# Evaluate best model
test_matrix <- model.matrix(LET_IS ~ ., data = MI_test)

#for (i in 1:19) {
#  coefi <- coef(regfit.best, id = i)
#  pred <- test.mat[, names(coefi)] %*% coefi 
#  val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
#}

```

### Question 3

Among your top predictors, do you see evidence of non-linear effects? How could you accommodate non-linear effects and still use a regularized regression approach? Does adding non-linear effects improve your model?

```{r non_linear, echo=FALSE}



```

### Question 4

Fit an appropriate Random Forest model. Report a comparison of performance to your linear model and explain any differences in performance. Do you see an important difference in how variables are used for predictions?

```{r random_forest, echo=FALSE}
library(randomForest)
set.seed(33)

# Random Forest model
forest_model <- randomForest(LET_IS ~ ., mtry= 12, data = MI_train, ntree = 500)

forest_pred <- predict(forest_model, MI_test, type = "class")
table(MI_test$LET_IS, forest_pred)

```

```{r performance, echo=FALSE}
# ROC curve
library(pROC)

roc_obj <- roc(MI_test$LET_IS, as.numeric(forest_pred))
plot(roc_obj, col = "blue")

# TODO: plot ROC curve for linear models

```

```{r}

```
